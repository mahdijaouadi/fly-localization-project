{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report,accuracy_score,precision_score,recall_score,mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(data):\n",
    "    x=torch.stack([t[0] for t in data])\n",
    "    return x\n",
    "def get_y(data):\n",
    "    y=torch.stack([torch.tensor(t[1]) for t in data])\n",
    "    return y\n",
    "with open(\"..//data//train_data.pkl\",'rb') as f:\n",
    "    train_data=pickle.load(f)\n",
    "\n",
    "x_train,y_train=get_x(train_data),get_y(train_data)\n",
    "y_train_classification,y_train_localization=y_train[:,0].to(torch.long),y_train[:,1:]\n",
    "del train_data\n",
    "with open(\"..//data//val_data.pkl\",'rb') as f:\n",
    "    val_data=pickle.load(f)\n",
    "x_val,y_val=get_x(val_data),get_y(val_data)\n",
    "y_val_classification,y_val_localization=y_val[:,0].to(torch.long),y_val[:,1:]\n",
    "del val_data\n",
    "\n",
    "# print(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a skip connection network\n",
    "\n",
    "device='cuda'\n",
    "class Block(nn.Module):\n",
    "    expansion=4\n",
    "    def __init__(self,in_channels,out_channels,downsample=None,stride=1):\n",
    "        super(Block,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels,out_channels,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(out_channels)\n",
    "        self.conv2=nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=stride,padding=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(out_channels)\n",
    "        self.conv3=nn.Conv2d(out_channels,out_channels*Block.expansion,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn3=nn.BatchNorm2d(out_channels*Block.expansion)\n",
    "        self.downsample=downsample\n",
    "    def forward(self,x):\n",
    "        identity=x\n",
    "        out=self.conv1(x)\n",
    "        out=self.bn1(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.conv2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=F.relu(out)\n",
    "        out=self.conv3(out)\n",
    "        out=self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            identity=self.downsample(identity)\n",
    "        out+=identity\n",
    "        out=F.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self,layers,output_size):\n",
    "        super(ResNet,self).__init__()\n",
    "        self.in_channels=64\n",
    "        self.conv1=nn.Conv2d(1,self.in_channels,kernel_size=7,stride=2,padding=3,bias=False)\n",
    "        self.bn1= nn.BatchNorm2d(self.in_channels)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=3,stride=2,padding=1)   \n",
    "\n",
    "        #Define Block layers\n",
    "        self.layer1=self._make_layer(layers[0],64,1)\n",
    "        self.layer2=self._make_layer(layers[1],128,2)\n",
    "        self.layer3=self._make_layer(layers[2],256,2)\n",
    "        self.layer4=self._make_layer(layers[3],512,2)\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.fc=nn.Linear(64*Block.expansion,output_size)\n",
    "\n",
    "    def _make_layer(self,num_blocks,out_channels,stride=1):\n",
    "        downsample=None\n",
    "        if stride!=1 or self.in_channels!=out_channels*Block.expansion:\n",
    "            downsample=nn.Sequential(nn.Conv2d(self.in_channels,out_channels*Block.expansion,kernel_size=1,stride=stride,padding=0,bias=False),\n",
    "                                     nn.BatchNorm2d(out_channels*Block.expansion))\n",
    "        layers=[]\n",
    "        layers.append(Block(in_channels=self.in_channels,out_channels=out_channels,downsample=downsample,stride=stride))\n",
    "        self.in_channels=out_channels*Block.expansion\n",
    "        for _ in range(num_blocks-1):\n",
    "            layers.append(Block(in_channels=self.in_channels,out_channels=out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, inputs):\n",
    "        inputs=inputs.to(device)\n",
    "        out=self.conv1(inputs)\n",
    "        \n",
    "        out=self.bn1(out)\n",
    "        out=self.maxpool1(out)\n",
    "        out=self.layer1(out)\n",
    "        out=self.layer2(out)\n",
    "        out=self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.avgpool(out)\n",
    "        out=torch.flatten(out,1)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "\n",
    "layers=[3,4,6,3]\n",
    "output_size=3\n",
    "model=ResNet(layers,output_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(x):\n",
    "    BATCH_SIZE=100\n",
    "    classification_predictions=torch.tensor([])\n",
    "    localization_predictions=torch.tensor([])\n",
    "    model.eval()\n",
    "    for i in range(0,len(x),BATCH_SIZE):\n",
    "        y_hat=model(x[i:i+BATCH_SIZE]).to('cpu')\n",
    "        y_hat_classification=F.softmax(y_hat[:,0])\n",
    "        y_hat_localization=y_hat[:,1:]\n",
    "        _,pred=torch.max(y_hat_classification,1)\n",
    "        classification_predictions=torch.cat((classification_predictions,pred),dim=0)\n",
    "        localization_predictions=torch.cat((localization_predictions,y_hat_localization),dim=0)\n",
    "        torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    return classification_predictions.to(torch.long),localization_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10\n",
    "MINI_BATCH_SIZE=32\n",
    "accumalation_step=1\n",
    "loss_classification_fn=nn.CrossEntropyLoss()\n",
    "loss_localization_fn=nn.MSELoss()\n",
    "optimizer=torch.optim.adam(model.parameters(),lr=1e-2)\n",
    "\n",
    "\n",
    "train_loss_progress=[]\n",
    "train_loss_hist=[]\n",
    "val_loss_hist=[]\n",
    "\n",
    "training_accuracy_hist=[]\n",
    "val_accuracy_hist=[]\n",
    "\n",
    "training_mae_hist=[]\n",
    "val_mae_hist=[]\n",
    "for epoch in range(EPOCHS):\n",
    "    model.eval()\n",
    "\n",
    "    classification_predictions,localization_predictions=get_predictions(x_val)\n",
    "    val_acc=accuracy_score(y_val.numpy(), classification_predictions.numpy())\n",
    "    val_mae=mean_absolute_error(y_val.numpy(),localization_predictions.numpy())\n",
    "    val_mae_hist.append(val_mae)\n",
    "    val_accuracy_hist.append(val_acc)\n",
    "\n",
    "\n",
    "    classification_predictions,localization_predictions=get_predictions(x_train)\n",
    "    training_acc=accuracy_score(y_train.numpy(), classification_predictions.numpy())\n",
    "    training_mae=mean_absolute_error(y_train.numpy(),localization_predictions.numpy())\n",
    "    training_mae_hist.append(training_mae)\n",
    "    training_accuracy_hist.append(training_acc)\n",
    "    print(training_acc,val_acc)\n",
    "    print(training_mae,val_mae)\n",
    "    model.train()\n",
    "    for i in range(0,len(x_train),MINI_BATCH_SIZE):\n",
    "        y_hat=model(x_train[i:i+MINI_BATCH_SIZE]).to('cpu')\n",
    "        classification_loss=loss_classification_fn(y_hat[:,0],y_train_classification[i:i+MINI_BATCH_SIZE])\n",
    "        localization_loss=loss_localization_fn(y_hat[:,1:],y_train_localization[i:i+MINI_BATCH_SIZE])\n",
    "        loss=loss/accumalation_step\n",
    "        loss.backward()\n",
    "        if (i+1)% accumalation_step==0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_loss_hist, label='List 1',color='blue')\n",
    "# plt.plot(val_loss_hist, label='List 2',color='red')\n",
    "# plt.show()\n",
    "plt.plot(training_accuracy_hist[0:], label='List 1',color='blue')\n",
    "plt.plot(val_accuracy_hist[0:], label='List 2',color='red')\n",
    "plt.show()\n",
    "\n",
    "classification_predictions,localization_predictions=get_predictions(x_val)\n",
    "report = classification_report(y_val.numpy(), classification_predictions.numpy(), target_names=[str(i) for i in range(200)])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
